{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8 - PySpark ML\n",
    "\n",
    "<p> In this lab, you will be tasked to model the NOAA weather data to make predictions.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the NOAA weather data to complete a machine learning prediction task. [Full documentation of the dataset here](https://www.ncei.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"matplotlib==3.2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we only use the weather data from CHICAGO_MIDWAY_AP_3SW Station.\n",
    "\n",
    "You can read the complete Weather Station List [here](https://data.giss.nasa.gov/gistemp/station_data_v4_globe/v4.temperature.inv.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('s3://noaa-ghcn-pds/csv/by_station/USC00111577.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, convert `DATE` to datetime type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"DATE\", F.to_date(df[\"DATE\"], \"yyyyMMdd\"))\n",
    "df.filter(df[\"DATE\"] == '1928-02-29').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `DATA_VALUE` to integer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"DATA_VALUE\", F.col(\"DATA_VALUE\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to re-organize the data such that each unique value in the `ELEMENT` column becomes a new column. We use groupby and pivot in PySpark to do the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df.groupBy(\"DATE\")\\\n",
    "    .pivot(\"ELEMENT\")\\\n",
    "    .agg(F.sum(\"DATA_VALUE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this exercise, we only use the core elements for the ML task. Feel free to explore other elements.\n",
    "\n",
    "- PRCP = Precipitation (tenths of mm)\n",
    "\n",
    "- SNOW = Snowfall (mm)\n",
    "\n",
    "- SNWD = Snow depth (mm)\n",
    "\n",
    "- TMAX = Maximum temperature (tenths of degrees C)\n",
    "\n",
    "- TMIN = Minimum temperature (tenths of degrees C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df = pivot_df.select(\n",
    "   \"DATE\", \"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"\n",
    ").withColumnRenamed(\"DATE\", \"date\")\\\n",
    " .withColumnRenamed(\"PRCP\", \"precip\")\\\n",
    " .withColumnRenamed(\"SNOW\", \"snow\")\\\n",
    " .withColumnRenamed(\"SNWD\", \"snow_depth\")\\\n",
    " .withColumnRenamed(\"TMAX\", \"temp_max\")\\\n",
    " .withColumnRenamed(\"TMIN\", \"temp_min\")\n",
    "\n",
    "core_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df.filter(core_df[\"date\"] == '1928-02-29').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = core_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in core_df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df = core_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform temperatures from tenths of degrees C to degree C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the temperatures across different years by taking the average of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df = core_df.withColumn(\"year\", F.year(F.col(\"date\")))\n",
    "\n",
    "yearly_avg_temp = core_df.groupBy(\"year\").agg(\n",
    "    F.avg(F.col(\"temp_max\")).alias(\"avg_temp_max\"),\n",
    "    F.avg(F.col(\"temp_min\")).alias(\"avg_temp_min\")\n",
    ")\n",
    "\n",
    "# collect data back to driver\n",
    "yearly_data = yearly_avg_temp.sort(\"year\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot yearly trends of temperatures\n",
    "years = [row['year'] for row in yearly_data]\n",
    "avg_temp_max = [row['avg_temp_max'] for row in yearly_data]\n",
    "avg_temp_min = [row['avg_temp_min'] for row in yearly_data]\n",
    "\n",
    "plt.plot(years, avg_temp_max, label='max temperature')\n",
    "plt.plot(years, avg_temp_min, label='min temperature')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Modeling \n",
    "\n",
    "Use the 5 core values to do regression prediction on the maximum temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = core_df.filter(F.col('date') <= F.to_date(F.lit('2020-12-31')))\n",
    "test_df = core_df.filter((F.col('date') > F.to_date(F.lit('2020-12-31'))) & (F.col('date') <= F.to_date(F.lit('2024-05-03'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the assembler to transform the `precip`, `snow`, `snow_depth`, \n",
    "# and `temp_min` to the form that can be handled by pyspark.ml. \n",
    "# Make the `outputCol` as `features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training set and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = train_df.select(F.col('features'), F.col('temp_max').alias('label'))\n",
    "final_test_df = test_df.select(F.col('features'), F.col('temp_max').alias('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Ridge Regression model (when ElasticNetParam set to 0, the model will use only L2 regularization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = LinearRegression(elasticNetParam=0, solver=\"auto\")\n",
    "\n",
    "# Define the parameter grid. Feel free to add more parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(ridge.regParam, [0.1, 0.01, 0.001]) \\\n",
    "    .addGrid(ridge.maxIter, [100, 500]) \\\n",
    "    .build()\n",
    "\n",
    "# TODO: Create the evaluator with RMSE as the evaluation metric\n",
    "evaluator = ...\n",
    "\n",
    "# Setup 5-fold cross-validation\n",
    "crossval = CrossValidator(estimator=ridge,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "# TODO: Fit the model on training data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions on the testing data\n",
    "\n",
    "\n",
    "# TODO: Evaluate the model\n",
    "rmse = ...\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "\n",
    "# TODO: Display 5 examples of predicted values and labels for comparison\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
